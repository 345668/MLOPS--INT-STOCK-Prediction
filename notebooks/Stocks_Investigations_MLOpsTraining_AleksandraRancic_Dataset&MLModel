import yfinance as yf
import os
from datetime import datetime
import numpy as np

"""
This function downloads historical stock price data for a specified stock label. 
It downloads the data within a given time range using the Yahoo Finance API.

This function will use the yf.download method from the Yahoo Finance library, 
and will return the adjusted closing price.
"""

folder_path = "/Users/aleksandra.rancic/Desktop/MLOps/dataset"

def download_dataset(label, start_date):
    
    end_date = datetime.now().strftime('%Y-%m-%d')
    
    stock_data = yf.download(label, start = start_date, end = end_date)
    
    file_name = f"{label}_stock_data.csv"
    
    file_path = os.path.join(folder_path, file_name)
    
    stock_data.to_csv(file_path)
    
    return stock_data

# label = the stock label symbol ('AAPL' for Apple Inc.)
# start_date = the starting date for the historical data 
# end_date = the ending date for the historical data
# The format for dates is 'YYYY-MM-DD'.

#Download stock data
label = 'AAPL'
start_date = '2014-09-12'
adj_close_data = download_dataset(label, start_date)

import pandas as pd

df = pd.read_csv('/Users/aleksandra.rancic/Desktop/MLOps/dataset/AAPL_stock_data.csv')
df.head()

def preprocess_data(data, sequence_length):
    """
    Preprocesses the raw stock price data and creates input sequences for LSTM model.

    Parameters:
    -----------
    data: array-like
        The stock price data (or any time series data) to preprocess.

    sequence_length: int
        The number of previous time steps to consider in each input sequence.

    Returns:
    --------
    sequences: list of tuples
        Each tuple contains:
        - sequence: the input sequence for the LSTM model.
        - target: the next value after the sequence, used for prediction.
    """
    sequences = []
    for i in range(len(data) - sequence_length):
        # Direct slicing for both NumPy arrays and DataFrames
        sequence = data[i:i+sequence_length]
        target = data[i+sequence_length]
        sequences.append((sequence, target))

    return sequences
import numpy as np
from sklearn.preprocessing import MinMaxScaler

def split_and_scale_data(data, test_size = 0.2, sequence_length = 60):
    """
    This function splits a time series dataset into training and testing sets, scales the data, 
    and creates input sequences for LSTM model training and testing.

    Parameters:
    -----------
    data: array-like
        The stock price (or any time series data) that you want to split and preprocess.
        Typically, this will be a Pandas DataFrame or NumPy array with one or more columns.

    test_size: float, optional
        The proportion of the data to reserve for testing. Default is 0.2 (20% test, 80% train).

    sequence_length: int, optional
        The number of previous time steps to consider in each input sequence. This defines how much
        historical data the model will use to predict the next value. Defalut is 60 (usually 60 days).

    Returns:
    --------
    x_train: numpy array
        The input sequences for the LSTM model from the training dataset. Each sequence contains 
        'sequence_length' time steps of stock prices.
    y_train: numpy array
        The corresponding target values for each sequence in the training dataset. These are the values
        the model will learn to predict.
    x_test: numpy array
        The input sequences for the LSTM model from the test dataset.
    y_test: numpy array
        The corresponding target values for the test dataset.
    scaler: MinMaxScaler object
        The scaler object used to scale the data, allowing you to inverse-transform the data later
        for interpretation after predictions.


    Notes:
    -------
    The function scales the entire dataset between 0 and 1 using MinMaxScaler before splitting
    into training and testing sets. LSTMs require normalized data for optimal performance. 
    """
    
    scaler = MinMaxScaler(feature_range = (0, 1))
    scaled_data = scaler.fit_transform(data)

    train_size = int(len(scaled_data)*(1-test_size))
    train_data = scaled_data[:train_size]
    test_data = scaled_data[train_size:]

    train_sequences = preprocess_data(train_data, sequence_length)
    test_sequences = preprocess_data(test_data, sequence_length)

    x_train = np.array([seq[0] for seq in train_sequences])
    y_train = np.array([seq[1] for seq in train_sequences])
    x_test = np.array([seq[0] for seq in test_sequences])
    y_test = np.array([seq[1] for seq in test_sequences])

    return x_train, y_train, x_test, y_test, scaler

from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout

def train_lstm_model(X_train, y_train, sequence_length, n_features = 1, epochs = 20, batch_size = 32):
    """
    This function trains an LSTM model on the given time series data.
    Returns = the trained LSTM model that can be used for predictions of new data.
    """
    model = Sequential()

    model.add(LSTM(units = 50, return_sequences = True, input_shape = (sequence_length, n_features)))
    model.add(Dropout(0.2)) #To prevent overfitting

    model.add(LSTM(units = 50, return_sequences = False))
    model.add(Dropout(0.2))

    model.add(Dense(units = 1))
    model.compile(optimizer = 'adam', loss = 'mean_squared_error')

    model.fit(X_train, y_train, epochs = epochs, batch_size = batch_size)

    return model

import matplotlib.pyplot as plt
import seaborn as sns

def explore_data(df):
    print("Data Overview:")
    print(df.head())
    print("\nData Description:")
    print(df.describe())
    print("\nMissing Values:")
    print(df.isnull().sum())

explore_data(df)

def visualize_data(df):
    plt.figure(figsize = (10, 6))
    plt.plot(df['Date'], df['Adj Close'], label = "Adjusted Close Price", color = 'blue')
    plt.title('Apple Stock Adjusted Close Price Over Time')
    plt.xlabel('Date')
    plt.ylabel('Adjusted Close Price')
    plt.legend()
    plt.grid(True)
    plt.show();

    numeric_df = df.drop(columns = ['Date'])

    plt.figure(figsize = (10, 6))
    sns.heatmap(numeric_df.corr(), annot = True, cmap = 'coolwarm')
    plt.title('Correlation Heatmap of Stock Features')
    plt.show();

visualize_data(df)

sequence_length = 60
x_train, y_train, x_test, y_test, scaler = split_and_scale_data(df[['Adj Close']].values, test_size=0.2, sequence_length=sequence_length)

lstm_model = train_lstm_model(x_train, y_train, sequence_length, n_features = 1, epochs = 20, batch_size = 32)

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

def evaluate_model(model, X_test, y_test, scaler):
    # Make predictions on the test set
    predictions = model.predict(X_test)

    # Reverse the scaling on predictions and true values
    predictions_rescaled = scaler.inverse_transform(predictions)
    y_test_rescaled = scaler.inverse_transform(y_test.reshape(-1, 1))

    # Compute evaluation metrics
    mse = mean_squared_error(y_test_rescaled, predictions_rescaled)
    mae = mean_absolute_error(y_test_rescaled, predictions_rescaled)
    rmse = np.sqrt(mse)
    mape = np.mean(np.abs((y_test_rescaled - predictions_rescaled) / y_test_rescaled)) * 100
    r2 = r2_score(y_test_rescaled, predictions_rescaled)

    # Print evaluation metrics
    print(f"Mean Squared Error (MSE): {mse}")
    print(f"Root Mean Squared Error (RMSE): {rmse}")
    print(f"Mean Absolute Error (MAE): {mae}")
    print(f"Mean Absolute Percentage Error (MAPE): {mape}%")
    print(f"R-squared (R²): {r2}")

    metrics = {
        "Mean Squared Error (MSE)": mse,
        "Root Mean Squared Error (RMSE)": rmse,
        "Mean Absolute Error (MAE)": mae,
        "Mean Absolute Percentage Error (MAPE)": mape,
        "R-squared (R²)": r2
    }
    # Save metrics to a JSON file
         with open(filename, 'w') as f:
            json.dump(metrics, f)

        print(f"Metrics saved to {filename}")
    
        return predictions_rescaled, mse, mae, rmse, mape, r2

# Call the evaluation function
predictions_rescaled, mse, mae, rmse, mape, r2 = evaluate_model(lstm_model, x_test, y_test, scaler)

# Function to make predictions on new data
def make_predictions(model, X_test):
    predictions = model.predict(X_test)
    predictions_rescaled = scaler.inverse_transform(predictions)
    return predictions_rescaled

# Example: Make predictions on test data
predictions = make_predictions(lstm_model, x_test)

# Visualization of the predictions vs true values
def plot_predictions(predictions, y_test, scaler):
    y_test_rescaled = scaler.inverse_transform(y_test.reshape(-1, 1))

    plt.figure(figsize=(10, 6))
    plt.plot(y_test_rescaled, color='blue', label='True Values')
    plt.plot(predictions, color='red', label='Predicted Values')
    plt.title('True vs Predicted Stock Prices')
    plt.xlabel('Time Steps')
    plt.ylabel('Stock Price')
    plt.legend()
    plt.show()

# Call the function to plot predictions vs true values
plot_predictions(predictions_rescaled, y_test, scaler)

from keras.models import load_model
import joblib

# Save the model using Keras
lstm_model.save('lstm_model.h5')

# Load the model to ensure it can be used after saving
loaded_model = load_model('lstm_model.h5')

# Save the loaded Keras model as a pickle file
joblib.dump(loaded_model, 'lstm_model.pkl')

# Optionally, confirm the model is saved by loading it back
model_from_pkl = joblib.load('lstm_model.pkl')

# Save the scaler to a file
joblib.dump(scaler, 'minmax_scaler.pkl') 

