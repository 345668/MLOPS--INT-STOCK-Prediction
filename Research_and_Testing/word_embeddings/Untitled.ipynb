{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8cc73fb-0f0f-4d08-8dbf-2e8b1cec54ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from scipy.spatial.distance import cdist, cosine\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import seaborn as sns\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "from sklearn.decomposition import IncrementalPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6062b7da-78c2-47a8-a4df-0859bf49c884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained FastText model...\n",
      "Pre-trained FastText model loaded.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import FastText\n",
    "\n",
    "# Path to FastText pre-trained Wikipedia model\n",
    "fasttext_wiki_file = 'data/fasttext/wiki-news-300d-1M.vec'\n",
    "\n",
    "# Load pre-trained FastText model (in Gensim format)\n",
    "print(\"Loading pre-trained FastText model...\")\n",
    "fasttext_model = KeyedVectors.load_word2vec_format(fasttext_wiki_file, binary=False)\n",
    "print(\"Pre-trained FastText model loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423771da-ec73-4f87-9c25-76880455e4db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ba4f9e7-9403-44f4-9254-683e05b8fb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'is', 'a', 'sample', 'sentence'], ['we', 'are', 'fine-tuning', 'the', 'fasttext', 'model'], ['domain', 'specific', 'data', 'helps', 'improve', 'performance'], ['stock', 'market', 'crash', 'leads', 'to', 'recession'], ['equity', 'prices', 'are', 'rising', 'due', 'to', 'inflation'], ['capital', 'gains', 'are', 'taxable', 'in', 'many', 'countries'], ['bull', 'market', 'is', 'typically', 'associated', 'with', 'growth'], ['bear', 'market', 'refers', 'to', 'declining', 'asset', 'prices'], ['dividends', 'are', 'a', 'portion', 'of', 'profits', 'distributed', 'to', 'shareholders'], ['bond', 'yields', 'are', 'inversely', 'proportional', 'to', 'bond', 'prices'], ['hedge', 'funds', 'use', 'leverage', 'to', 'maximize', 'returns'], ['credit', 'default', 'swap', 'protects', 'against', 'debt', 'default'], ['derivative', 'instruments', 'derive', 'value', 'from', 'underlying', 'assets'], ['central', 'banks', 'control', 'monetary', 'policy', 'through', 'interest', 'rates'], ['technical', 'analysis', 'involves', 'charting', 'stock', 'price', 'movements'], ['candlestick', 'patterns', 'are', 'used', 'to', 'predict', 'market', 'trends'], ['moving', 'averages', 'smooth', 'out', 'price', 'data', 'for', 'trend', 'identification'], ['relative', 'strength', 'index', 'measures', 'overbought', 'or', 'oversold', 'conditions'], ['fibonacci', 'retracement', 'helps', 'identify', 'potential', 'support', 'levels'], ['breakout', 'traders', 'focus', 'on', 'stocks', 'breaking', 'through', 'resistance'], ['stop', 'loss', 'orders', 'limit', 'potential', 'losses', 'on', 'trades'], ['limit', 'orders', 'allow', 'traders', 'to', 'set', 'buy', 'or', 'sell', 'price', 'points'], ['volume', 'analysis', 'is', 'used', 'to', 'confirm', 'market', 'momentum'], ['day', 'trading', 'involves', 'buying', 'and', 'selling', 'stocks', 'within', 'the', 'same', 'day'], ['swing', 'trading', 'captures', 'short', 'to', 'medium', 'term', 'market', 'moves'], ['price', 'to', 'earnings', 'ratio', 'is', 'a', 'valuation', 'metric', 'for', 'stocks'], ['market', 'capitalization', 'refers', 'to', 'the', 'total', 'value', 'of', 'a', 'company’s', 'outstanding', 'shares'], ['blue', 'chip', 'stocks', 'are', 'considered', 'stable', 'and', 'profitable', 'long-term', 'investments'], ['penny', 'stocks', 'are', 'low-priced', 'stocks', 'with', 'high', 'volatility'], ['initial', 'public', 'offering', 'occurs', 'when', 'a', 'company', 'goes', 'public'], ['short', 'selling', 'involves', 'borrowing', 'shares', 'to', 'sell', 'in', 'anticipation', 'of', 'price', 'declines'], ['margin', 'trading', 'allows', 'investors', 'to', 'borrow', 'money', 'to', 'buy', 'stocks'], ['options', 'contracts', 'give', 'traders', 'the', 'right', 'but', 'not', 'the', 'obligation', 'to', 'buy', 'or', 'sell', 'assets'], ['call', 'option', 'contracts', 'allow', 'the', 'holder', 'to', 'buy', 'at', 'a', 'specific', 'price'], ['put', 'option', 'contracts', 'allow', 'the', 'holder', 'to', 'sell', 'at', 'a', 'specific', 'price'], ['earnings', 'season', 'is', 'when', 'companies', 'release', 'their', 'quarterly', 'financial', 'results'], ['economic', 'indicators', 'such', 'as', 'GDP', 'and', 'unemployment', 'rates', 'affect', 'market', 'behavior'], ['market', 'sentiment', 'reflects', 'the', 'overall', 'attitude', 'of', 'investors', 'toward', 'a', 'particular', 'market'], ['algorithmic', 'trading', 'uses', 'computer', 'programs', 'to', 'execute', 'trades', 'based', 'on', 'predefined', 'criteria'], ['high-frequency', 'trading', 'involves', 'making', 'millions', 'of', 'trades', 'in', 'fractions', 'of', 'a', 'second'], ['risk', 'management', 'is', 'key', 'to', 'protecting', 'capital', 'in', 'volatile', 'markets'], ['portfolio', 'diversification', 'reduces', 'the', 'risk', 'by', 'investing', 'in', 'varied', 'assets'], ['beta', 'measures', 'a', 'stock’s', 'volatility', 'relative', 'to', 'the', 'overall', 'market']]\n"
     ]
    }
   ],
   "source": [
    "# Example: Prepare a custom corpus (can be from your domain-specific dataset)\n",
    "custom_corpus = [\n",
    "    ['this', 'is', 'a', 'sample', 'sentence'],\n",
    "    ['we', 'are', 'fine-tuning', 'the', 'fasttext', 'model'],\n",
    "    ['domain', 'specific', 'data', 'helps', 'improve', 'performance'],\n",
    "    ['stock', 'market', 'crash', 'leads', 'to', 'recession'],\n",
    "    ['equity', 'prices', 'are', 'rising', 'due', 'to', 'inflation'],\n",
    "    ['capital', 'gains', 'are', 'taxable', 'in', 'many', 'countries'],\n",
    "    ['bull', 'market', 'is', 'typically', 'associated', 'with', 'growth'],\n",
    "    ['bear', 'market', 'refers', 'to', 'declining', 'asset', 'prices'],\n",
    "    ['dividends', 'are', 'a', 'portion', 'of', 'profits', 'distributed', 'to', 'shareholders'],\n",
    "    ['bond', 'yields', 'are', 'inversely', 'proportional', 'to', 'bond', 'prices'],\n",
    "    ['hedge', 'funds', 'use', 'leverage', 'to', 'maximize', 'returns'],\n",
    "    ['credit', 'default', 'swap', 'protects', 'against', 'debt', 'default'],\n",
    "    ['derivative', 'instruments', 'derive', 'value', 'from', 'underlying', 'assets'],\n",
    "    ['central', 'banks', 'control', 'monetary', 'policy', 'through', 'interest', 'rates'],\n",
    "    ['technical', 'analysis', 'involves', 'charting', 'stock', 'price', 'movements'],\n",
    "    ['candlestick', 'patterns', 'are', 'used', 'to', 'predict', 'market', 'trends'],\n",
    "    ['moving', 'averages', 'smooth', 'out', 'price', 'data', 'for', 'trend', 'identification'],\n",
    "    ['relative', 'strength', 'index', 'measures', 'overbought', 'or', 'oversold', 'conditions'],\n",
    "    ['fibonacci', 'retracement', 'helps', 'identify', 'potential', 'support', 'levels'],\n",
    "    ['breakout', 'traders', 'focus', 'on', 'stocks', 'breaking', 'through', 'resistance'],\n",
    "    ['stop', 'loss', 'orders', 'limit', 'potential', 'losses', 'on', 'trades'],\n",
    "    ['limit', 'orders', 'allow', 'traders', 'to', 'set', 'buy', 'or', 'sell', 'price', 'points'],\n",
    "    ['volume', 'analysis', 'is', 'used', 'to', 'confirm', 'market', 'momentum'],\n",
    "    ['day', 'trading', 'involves', 'buying', 'and', 'selling', 'stocks', 'within', 'the', 'same', 'day'],\n",
    "    ['swing', 'trading', 'captures', 'short', 'to', 'medium', 'term', 'market', 'moves'],\n",
    "    ['price', 'to', 'earnings', 'ratio', 'is', 'a', 'valuation', 'metric', 'for', 'stocks'],\n",
    "    ['market', 'capitalization', 'refers', 'to', 'the', 'total', 'value', 'of', 'a', 'company’s', 'outstanding', 'shares'],\n",
    "    ['blue', 'chip', 'stocks', 'are', 'considered', 'stable', 'and', 'profitable', 'long-term', 'investments'],\n",
    "    ['penny', 'stocks', 'are', 'low-priced', 'stocks', 'with', 'high', 'volatility'],\n",
    "    ['initial', 'public', 'offering', 'occurs', 'when', 'a', 'company', 'goes', 'public'],\n",
    "    ['short', 'selling', 'involves', 'borrowing', 'shares', 'to', 'sell', 'in', 'anticipation', 'of', 'price', 'declines'],\n",
    "    ['margin', 'trading', 'allows', 'investors', 'to', 'borrow', 'money', 'to', 'buy', 'stocks'],\n",
    "    ['options', 'contracts', 'give', 'traders', 'the', 'right', 'but', 'not', 'the', 'obligation', 'to', 'buy', 'or', 'sell', 'assets'],\n",
    "    ['call', 'option', 'contracts', 'allow', 'the', 'holder', 'to', 'buy', 'at', 'a', 'specific', 'price'],\n",
    "    ['put', 'option', 'contracts', 'allow', 'the', 'holder', 'to', 'sell', 'at', 'a', 'specific', 'price'],\n",
    "    ['earnings', 'season', 'is', 'when', 'companies', 'release', 'their', 'quarterly', 'financial', 'results'],\n",
    "    ['economic', 'indicators', 'such', 'as', 'GDP', 'and', 'unemployment', 'rates', 'affect', 'market', 'behavior'],\n",
    "    ['market', 'sentiment', 'reflects', 'the', 'overall', 'attitude', 'of', 'investors', 'toward', 'a', 'particular', 'market'],\n",
    "    ['algorithmic', 'trading', 'uses', 'computer', 'programs', 'to', 'execute', 'trades', 'based', 'on', 'predefined', 'criteria'],\n",
    "    ['high-frequency', 'trading', 'involves', 'making', 'millions', 'of', 'trades', 'in', 'fractions', 'of', 'a', 'second'],\n",
    "    ['risk', 'management', 'is', 'key', 'to', 'protecting', 'capital', 'in', 'volatile', 'markets'],\n",
    "    ['portfolio', 'diversification', 'reduces', 'the', 'risk', 'by', 'investing', 'in', 'varied', 'assets'],\n",
    "    ['beta', 'measures', 'a', 'stock’s', 'volatility', 'relative', 'to', 'the', 'overall', 'market']\n",
    "]\n",
    "# Ensure the corpus is tokenized properly (if needed)\n",
    "def tokenize_sentence(sentence):\n",
    "    if isinstance(sentence, str):\n",
    "        return sentence.lower().split()  # Tokenize if it's a string\n",
    "    return sentence  # Return unchanged if it's already a list\n",
    "\n",
    "# Apply tokenization only if necessary\n",
    "custom_corpus = [tokenize_sentence(sentence) for sentence in custom_corpus]\n",
    "\n",
    "# Check the result\n",
    "print(custom_corpus)\n",
    "\n",
    "\n",
    "# Apply tokenization to your custom corpus (if it isn't tokenized yet)\n",
    "custom_corpus = [tokenize_sentence(sentence) for sentence in custom_corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb585703-ad1e-4639-a25b-4941dc995a6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1643194d-7d2a-45c7-9778-68b67a8ecf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocabulary...\n",
      "Fine-tuning the FastText model...\n",
      "Fine-tuning complete.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "# Ensure the custom_corpus is a list of lists, where each inner list is a tokenized sentence\n",
    "if not isinstance(custom_corpus, list) or not all(isinstance(sentence, list) for sentence in custom_corpus):\n",
    "    raise ValueError(\"custom_corpus must be a list of tokenized sentences (list of lists).\")\n",
    "\n",
    "# Initialize FastText model\n",
    "fasttext_model = FastText(vector_size=300)\n",
    "\n",
    "# Build vocabulary from the custom corpus\n",
    "print(\"Building vocabulary...\")\n",
    "fasttext_model.build_vocab(corpus_iterable=custom_corpus)\n",
    "\n",
    "# Fine-tune the FastText model on the custom corpus\n",
    "print(\"Fine-tuning the FastText model...\")\n",
    "fasttext_model.train(corpus_iterable=custom_corpus, total_examples=len(custom_corpus), epochs=5)\n",
    "print(\"Fine-tuning complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccac86a9-9f70-4506-b22b-dd118f23218b",
   "metadata": {},
   "outputs": [],
   "source": [
    "analogies_path = Path('data', 'analogies-en.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31a11bbf-eb18-4886-99fd-9cc9409ee62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned FastText model saved.\n",
      "   total_correct  total_incorrect  total_attempted  accuracy\n",
      "0              0                0                0         0\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "fasttext_model.save('fine_tuned_fasttext.model')\n",
    "print(\"Fine-tuned FastText model saved.\")\n",
    "\n",
    "# Load the saved model (if needed)\n",
    "fine_tuned_model = FastText.load('fine_tuned_fasttext.model')\n",
    "\n",
    "# Evaluate the fine-tuned model\n",
    "def eval_analogies_fasttext(model, vocab=30000):\n",
    "    analogies_result = model.wv.evaluate_word_analogies(analogies_path, restrict_vocab=vocab, case_insensitive=True)\n",
    "    correct = sum([len(section['correct']) for section in analogies_result[1]])\n",
    "    incorrect = sum([len(section['incorrect']) for section in analogies_result[1]])\n",
    "    return pd.DataFrame([{\n",
    "        'total_correct': correct,\n",
    "        'total_incorrect': incorrect,\n",
    "        'total_attempted': correct + incorrect,\n",
    "        'accuracy': correct / (correct + incorrect) if (correct + incorrect) > 0 else 0\n",
    "    }])\n",
    "\n",
    "# Evaluate fine-tuned FastText model\n",
    "fine_tuned_result = eval_analogies_fasttext(fine_tuned_model, vocab=100000)\n",
    "print(fine_tuned_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc3cf96-0a67-483e-976d-ae1a8dd3b738",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7110f4e6-b965-4b54-9023-e253753c85e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pretrained_fasttext_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.75\u001b[39m}  \u001b[38;5;66;03m# Example value, replace with actual evaluation result\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Evaluate pre-trained FastText model\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m fasttext_wiki_result \u001b[38;5;241m=\u001b[39m evaluate_analogy_accuracy(\u001b[43mpretrained_fasttext_model\u001b[49m)  \u001b[38;5;66;03m# Use your pre-trained model\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Evaluate fine-tuned FastText model\u001b[39;00m\n\u001b[0;32m     11\u001b[0m fine_tuned_result \u001b[38;5;241m=\u001b[39m evaluate_analogy_accuracy(fasttext_model)  \u001b[38;5;66;03m# Your fine-tuned model\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pretrained_fasttext_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Assuming you have a function `evaluate_analogy_accuracy(model)` that returns the accuracy\n",
    "def evaluate_analogy_accuracy(model):\n",
    "    # Implement your analogy evaluation logic here\n",
    "    # For simplicity, returning a mock result\n",
    "    return {'accuracy': 0.75}  # Example value, replace with actual evaluation result\n",
    "\n",
    "# Evaluate pre-trained FastText model\n",
    "fasttext_wiki_result = evaluate_analogy_accuracy(pretrained_fasttext_model)  # Use your pre-trained model\n",
    "\n",
    "# Evaluate fine-tuned FastText model\n",
    "fine_tuned_result = evaluate_analogy_accuracy(fasttext_model)  # Your fine-tuned model\n",
    "\n",
    "# Combine results\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': ['Pre-trained FastText', 'Fine-tuned FastText'],\n",
    "    'Accuracy': [fasttext_wiki_result['accuracy'], fine_tuned_result['accuracy']]\n",
    "})\n",
    "\n",
    "# Plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x='Model', y='Accuracy', data=results_df)\n",
    "plt.title('Pre-trained vs Fine-tuned FastText Model Analogy Task Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924cab41-1944-4c9d-ac17-851c980b7569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeaf3a8-40d5-4403-8337-63c7d8e1df56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aebb872-24b6-4897-bb93-13898e3df02c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f0709c-cb6b-4337-a8c7-4e07b0a083d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c767d2b1-78dd-4d63-a6c2-2aa061bbd6f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a950df-51ea-406e-b8f4-476e643bd707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3a9378-df08-49f6-91a0-efbae674a3bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b20220-9b5b-4ad4-8369-679f3d200f61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c03a53-8014-40cf-b607-64e1e3e269c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfef38c7-c2cc-412a-9d3c-2b18594c61e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a63128-a88c-45d6-b7b0-4888be68e27d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17835d3-7672-419b-9d1e-bec632380e93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0224374-17f5-40e3-9861-c9d8835b11c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718558ac-24a0-46fd-88d4-a4111567df5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b94b3e4-6f89-4167-9115-c810aa117f32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5e25cd-014b-4159-9703-7db71679cd2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea4c44e-874b-4d70-8397-dcd667e37d12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095ed5e5-377f-4e7c-b277-b647c95bcc6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef661cc6-1a82-41aa-9bcf-44b7a7b7aef9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad774fec-a3fb-487f-96eb-f14b35b37762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d71b2bd-9284-4765-8356-859d183bf109",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84805000-9988-4827-a5c8-c9ee1461fc19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (py311)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
